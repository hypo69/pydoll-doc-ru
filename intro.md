# Как работать с библиотекой Pydoll: Подробное руководство

Pydoll — это новая библиотека автоматизации браузеров на Python, 
разработанная для веб-скрейпинга, тестирования и автоматизации повторяющихся задач. 
Она отличается от традиционных инструментов, таких как Selenium или Playwright, тем, 
что не использует веб-драйвер. Вместо этого Pydoll напрямую подключается к браузерам через протокол DevTools (DevTools Protocol).

Pydoll, чей первый стабильный релиз, версия 1.0, вышел в феврале 2025 года, быстро набирает популярность в сообществе Python для веб-скрейпинга.


Его асинхронная архитектура, способность обрабатывать JavaScript-тяжелые веб-сайты и встроенный обход Cloudflare делают его отличным выбором 
для большинства современных задач скрейпинга. Следуя этому пошаговому руководству, вы сможете легко настроить и развернуть веб-скрейпер для сбора 
данных с динамических веб-сайтов и преодолеть распространенные проблемы, такие как блокировка IP-адресов и CAPTCHA.

*ДИСКЛЕЙМЕР* Всегда помните о юридических и этических аспектах веб-скрейпинга, а также используйте прокси и задержки, чтобы избежать перегрузки целевых серверов.


---

**Ключевые Особенности Pydoll:**

*   **Асинхронная архитектура (Async-first)**: Pydoll полностью построен на `asyncio`, что обеспечивает высокую конкурентность, эффективное использование памяти и поддержку современных шаблонов разработки на Python. Это позволяет обрабатывать тысячи одновременных подключений и выполнять неблокирующие операции, что критически важно для масштабируемого веб-скрейпинга.
*   **Человекоподобные взаимодействия**: Библиотека имитирует реалистичные действия пользователя, такие как ввод текста, движения мыши и клики, чтобы уменьшить вероятность обнаружения ботов.
*   **Встроенный обход Cloudflare**: Pydoll автоматически обходит антибот-защиту Cloudflare без использования сторонних сервисов, имитируя доверенные сессии браузера при высокой репутации IP-адреса. Он также имеет встроенную обработку CAPTCHA.
*   **Поддержка нескольких браузеров**: Совместим с Chrome, Edge и другими браузерами на основе Chromium через единый интерфейс.
*   **Параллельный скрейпинг**: Позволяет одновременно скрейпить несколько страниц или веб-сайтов, значительно сокращая общее время выполнения.
*   **Интеграция прокси**: Поддерживает использование прокси для ротации IP-адресов, геотаргетинга и обхода ограничений скорости во время скрейпинга.
*   **Перехват запросов**: Возможность перехватывать, изменять или блокировать HTTP-запросы и ответы для расширенной автоматизации.
*   **Скриншоты и экспорт в PDF**: Позволяет делать скриншоты целых страниц или конкретных элементов, а также генерировать высококачественные PDF-файлы.

---

В этом руководстве я расскажу, как эффективно использовать библиотеку Pydoll для веб-скрейпинга, 
включая работу с динамическими веб-сайтами, обход защиты Cloudflare и использование прокси-серверов.

### **Шаг 1: Настройка Проекта Pydoll**

Перед началом работы убедитесь, что у вас установлен Python 3+.

1.  **Создание папки проекта**:
    Откройте терминал или командную строку и выполните следующие команды, чтобы создать папку для вашего проекта и перейти в нее:

```bash
mkdir pydoll-scraper
cd pydoll-scraper
```
    Папка `pydoll-scraper` будет служить корневым каталогом вашего проекта.
2.  **Инициализация виртуальной среды**:
    Рекомендуется использовать виртуальную среду для изоляции зависимостей проекта. Внутри папки проекта выполните:

```bash
python -m venv venv
```
3.  **Активация виртуальной среды**:
    *   **На Linux или macOS**:

```bash
source venv/bin/activate
```
       
    *   **На Windows**:

```bash
venv\Scripts\activate
```
       
4.  **Создание файла скрейпера**:
    В вашей любимой Python IDE (например, Visual Studio Code или PyCharm) создайте пустой файл с именем `scraper.py` внутри папки проекта. Вскоре он будет содержать логику извлечения данных.

---

### **Шаг 2: Установка Pydoll**

Когда виртуальная среда активирована, установите Pydoll с помощью менеджера пакетов `pip`:

```bash
pip install pydoll-python
```

---

### **Шаг 3: Скрейпинг Данных с Динамического Веб-Сайта**

Мы будем использовать Pydoll для извлечения данных с асинхронной, JavaScript-ориентированной версии сайта "Quotes to Scrape". Этот сайт динамически отображает элементы цитат с небольшой задержкой, что делает его недоступным для традиционных инструментов скрейпинга "из коробки".

**Код `scraper.py` для базового скрейпинга:**

```python
import asyncio
from pydoll.browser.chrome import Chrome
from pydoll.constants import By
import csv

async def main():
    async with Chrome() as browser:
        # 1. Запуск браузера Chrome и открытие новой страницы
        await browser.start()
        page = await browser.get_page()

        # 2. Переход на целевую страницу
        # Параметр ?delay=2000 имитирует 2-секундную задержку загрузки данных
        await page.go_to("https://quotes.toscrape.com/js-delayed/?delay=2000")

        # 3. Ожидание появления HTML-элементов
        # Это важно для динамических сайтов, где контент загружается после AJAX-запросов
        await page.wait_element(By.CSS_SELECTOR, ".quote", timeout=3) # Ждать до 3 секунд

        # 4. Подготовка структуры данных для хранения scraped данных
        quotes = []

        # 5. Выбор всех элементов цитат
        quote_elements = await page.find_elements(By.CSS_SELECTOR, ".quote")

        # 6. Итерация по элементам и извлечение данных
        for quote_element in quote_elements:
            # Извлечение текста цитаты (удаляем лишние кавычки)
            text_element = await quote_element.find_element(By.CSS_SELECTOR, ".text")
            text = (await text_element.get_element_text()).replace("“", "").replace("”", "")

            # Извлечение автора
            author_element = await quote_element.find_element(By.CSS_SELECTOR, ".author")
            author = await author_element.get_element_text()

            # Извлечение всех связанных тегов
            tag_elements = await quote_element.find_elements(By.CSS_SELECTOR, ".tag")
            tags = [await tag_element.get_element_text() for tag_element in tag_elements]

            # Создание словаря для цитаты и добавление его в список
            quote = {
                "text": text,
                "author": author,
                "tags": tags
            }
            quotes.append(quote)

    # 7. Экспорт scraped данных в CSV
    with open("quotes.csv", "w", newline="", encoding="utf-8") as csvfile:
        fieldnames = ["text", "author", "tags"]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

        writer.writeheader() # Добавление заголовков
        for quote_data in quotes:
            # Преобразование списка тегов в строку для CSV
            quote_data["tags"] = ", ".join(quote_data["tags"])
            writer.writerow(quote_data)

# Запуск асинхронной функции скрейпинга
if __name__ == "__main__":
    asyncio.run(main())
```

**Разбор кода:**

1.  **Импорты и `main` функция**: Скрипт начинается с импорта необходимых модулей: `asyncio` для асинхронного программирования, `Chrome` из `pydoll.browser` для управления браузером, `By` из `pydoll.constants` для определения селекторов (например, CSS-селекторов) и `csv` для работы с CSV-файлами. Основная логика заключена в асинхронной функции `main()`.
2.  **Инициализация браузера**: `async with Chrome() as browser:` запускает экземпляр браузера Chrome. Это асинхронный контекстный менеджер, который обеспечивает правильное закрытие браузера после завершения работы. `await browser.start()` фактически запускает браузер, а `page = await browser.get_page()` получает объект страницы, с которой мы будем взаимодействовать.
3.  **Навигация**: `await page.go_to(...)` используется для перехода на указанный URL.
4.  **Ожидание элементов**: На динамических сайтах контент может загружаться с задержкой. Метод `await page.wait_element(By.CSS_SELECTOR, ".quote", timeout=3)` instructs Pydoll ждать, пока элементы, соответствующие CSS-селектору `.quote`, не появятся на странице, с таймаутом в 3 секунды. Это гарантирует, что мы пытаемся извлечь данные только после того, как они полностью загружены.
5.  **Поиск элементов**:
    *   `page.find_elements(By.CSS_SELECTOR, ".quote")` находит *все* элементы на странице, соответствующие заданному селектору, и возвращает их в виде списка.
    *   Внутри цикла, для каждого найденного `quote_element`, используются методы `quote_element.find_element()` для нахождения дочерних элементов, таких как текст цитаты (`.text`), автор (`.author`) и теги (`.tag`).
6.  **Извлечение текста**: `await element.get_element_text()` извлекает текстовое содержимое элемента. В данном случае, `.replace("“", "").replace("”", "")` используется для удаления фигурных кавычек из текста цитаты.
7.  **Хранение данных**: Извлеченные данные для каждой цитаты упаковываются в словарь, а затем добавляются в список `quotes`.
8.  **Экспорт в CSV**: После того как все цитаты собраны, данные экспортируются в файл `quotes.csv`. Используется модуль `csv.DictWriter`, который позволяет записывать словари напрямую в CSV-файл. Для поля `tags`, которое является списком, `", ".join(quote_data["tags"])` преобразует его в строку, разделенную запятыми, для лучшей читаемости в CSV.

**Запуск скрипта:**

В терминале, находясь в папке проекта с активированной виртуальной средой, выполните:

```bash
python scraper.py
```


После выполнения скрипта в вашей папке проекта появится файл `quotes.csv` с извлеченными данными.

---

### **Шаг 4: Обход Cloudflare с Pydoll**

Веб-приложения Firewalls (WAFs), такие как Cloudflare, часто используют сложные антибот-защиты, которые могут блокировать автоматизированные запросы. Pydoll предоставляет встроенные механизмы для обхода таких защит.

Pydoll предлагает два подхода для обхода Cloudflare:

1.  **Подход с использованием контекстного менеджера (Context Manager Approach)**:
    Этот метод обрабатывает антибот-защиту **синхронно**, приостанавливая выполнение скрипта до тех пор, пока защита не будет успешно пройдена.


```python
import asyncio
from pydoll.browser.chrome import Chrome
from pydoll.constants import By # Необходим для By.CSS_SELECTOR

async def main():
    async with Chrome() as browser:
        await browser.start()
        page = await browser.get_page()

        # Использование контекстного менеджера для автоматического обхода Cloudflare
        async with page.expect_and_bypass_cloudflare_captcha():
            # Подключение к странице, защищенной Cloudflare
            await page.go_to("https://www.scrapingcourse.com/antibot-challenge")
            print("Waiting for Cloudflare anti-bot to be handled...")

        # Этот код выполняется только после успешного обхода антибота
        print("Cloudflare anti-bot bypassed! Continuing with automation...")

        # Извлечение текста с успешной страницы
        await page.wait_element(By.CSS_SELECTOR, "#challenge-title", timeout=3)
        success_element = await page.find_element(By.CSS_SELECTOR, "#challenge-title")
        success_text = await success_element.get_element_text()
        print(success_text)

if __name__ == "__main__":
    asyncio.run(main())
```


    При запуске этого скрипта, окно Chrome будет автоматически обходить защиту и загружать целевую страницу.

2.  **Подход с фоновой обработкой (Background Processing Approach)**:
    Этот подход позволяет Pydoll обрабатывать антибот-защиту **асинхронно в фоновом режиме**, что дает вашему скрейперу возможность выполнять другие операции, пока происходит обход Cloudflare.


```python
import asyncio
from pydoll.browser.chrome import Chrome
from pydoll.constants import By

async def main():
    async with Chrome() as browser:
        await browser.start()
        page = await browser.get_page()

        # Включение фоновой обработки обхода Cloudflare
        await page.enable_auto_solve_cloudflare_captcha()

        # Подключение к странице, защищенной Cloudflare
        await page.go_to("https://www.scrapingcourse.com/antibot-challenge")
        print("Page loaded, Cloudflare anti-bot will be handled in the background...")

        # Здесь можно выполнять другие операции, пока обход CAPTCHA идет в фоне

        # Отключение автоматического решения антибота, когда оно больше не требуется
        await page.disable_auto_solve_cloudflare_captcha()

        # Извлечение текста с успешной страницы
        await page.wait_element(By.CSS_SELECTOR, "#challenge-title", timeout=3)
        success_element = await page.find_element(By.CSS_SELECTOR, "#challenge-title")
        success_text = await success_element.get_element_text()
        print(success_text)

if __name__ == "__main__":
    asyncio.run(main())
```


    В этом случае вывод скрипта покажет, что страница загружена, и обход Cloudflare будет обрабатываться в фоновом режиме.

**Важное замечание**: Обход Cloudflare может не всегда работать из-за таких факторов, как репутация IP-адреса или история навигации.

---

### **Шаг 5: Интеграция Вращающихся Прокси с Bright Data**

Если вы отправляете слишком много запросов с одного и того же IP-адреса, веб-сайт может заблокировать вас из-за ограничения скорости или других анти-скрейпинговых мер. Лучший способ избежать этого — **использовать прокси-серверы**. Прокси действует как посредник, скрывая ваш реальный IP-адрес и делая запросы от имени прокси-сервера.

Bright Data является одним из крупнейших поставщиков прокси-сетей, предлагая датацентровые, резидентные, ISP и мобильные прокси.

**Интеграция прокси в Pydoll:**

1.  **Получение учетных данных прокси Bright Data**:
    Вам потребуется получить хост, порт, имя пользователя и пароль вашего прокси из дашборда Bright Data. Из этих деталей вы сможете создать URL прокси:

```python
proxy_url = "<brightdata_proxy_username>: <brightdata_proxy_password>@<brightdata_proxy_host>:<brightdata_proxy_port>";
```
   

2.  **Конфигурация прокси в Pydoll (через `Options`)**:
    Pydoll позволяет интегрировать прокси, используя класс `Options` для передачи аргументов браузера.

```python
import asyncio
from pydoll.browser.chrome import Chrome
from pydoll.browser.options import Options # Импортируем Options
from pydoll.constants import By

async def main():
    # Создание объекта Options
    options = Options()

    # URL вашего прокси Bright Data. Замените заглушки на ваши реальные данные.
    proxy_url = "brd-customer-<customer_id>-zone-<zone_name>:<password>@<host>:<port>"
    # Пример: proxy_url = "brd-customer-xxxxxx-zone-residential:your_password@brd.superproxy.io:22225"

    # Конфигурация опции интеграции прокси
    options.add_argument(f"--proxy-server={proxy_url}")

    # Для избежания потенциальных SSL ошибок
    options.add_argument("--ignore-certificate-errors")

    # Запуск браузера с конфигурацией прокси
    async with Chrome(options=options) as browser:
        await browser.start()
        page = await browser.get_page()

        # Посещение специальной страницы, которая возвращает IP-адрес вызывающей стороны
        await page.go_to("https://httpbin.io/ip")

        # Извлечение содержимого страницы, содержащего только IP-адрес входящего запроса, и вывод его
        body_element = await page.find_element(By.CSS_SELECTOR, "body")
        body_text = await body_element.get_element_text()
        print(f"Current IP address: {body_text}")

if __name__ == "__main__":
    asyncio.run(main())
```


    Каждый раз при запуске этого скрипта вы будете видеть другой выходной IP-адрес благодаря ротации прокси Bright Data. Важно отметить, что менеджер прокси Pydoll поддерживает прокси, защищенные паролем, что обычно не поддерживается флагом `--proxy-server` Chrome.

3.  **Конфигурация прокси в Pydoll (через `page.set_proxy()`)**:
    Другой, более прямой способ настройки прокси для конкретной страницы:

```python
# ... внутри функции main() после получения объекта page ...
await page.set_proxy({
    "host": "brd.superproxy.io", # Пример хоста Bright Data
    "port": 33335,               # Пример порта Bright Data
    "username": "your_username", # Ваши учетные данные
    "password": "your_password"  # Ваши учетные данные
})
# Теперь все запросы с этой страницы будут проходить через указанный прокси
await page.go_to("https://httpbin.io/ip")
# ...
```
   


---

### **Ограничения Pydoll для Веб-Скрейпинга**

Хотя Pydoll является мощным инструментом, важно понимать его ограничения:

*   **Ограничение скорости (Rate Limiting)**: Даже с прокси-серверами вы все равно можете столкнуться с ограничением скорости, если отправляете слишком много запросов слишком быстро. Важно внедрять стратегии, такие как рандомизация задержек между запросами, чтобы избежать блокировки.
*   **CAPTCHA**: Хотя Pydoll может обходить Cloudflare, он не всегда может справиться со всеми типами CAPTCHA.
*   **Совместимость**: Pydoll разработан в первую очередь для браузеров на основе Chromium (Chrome, Edge). Он может не работать со всеми веб-страницами, особенно теми, которые требуют использования не-Chromium браузеров.

---

### **Альтернативы Pydoll для Веб-Скрейпинга**

Если Pydoll не подходит для ваших конкретных нужд, есть несколько других инструментов, которые стоит рассмотреть:

*   **SeleniumBase**: Фреймворк на Python, построенный на основе Selenium/WebDriver API, предлагающий профессиональный набор инструментов для веб-автоматизации и скрейпинга.
*   **Undetected ChromeDriver**: Модифицированная версия ChromeDriver, разработанная для обхода обнаружения популярными антибот-сервисами, такими как Imperva, DataDome и Distil Networks.
*   **Selenium**: Самый популярный инструмент для веб-скрейпинга, обеспечивающий полную автоматизацию браузера. Поддерживает все браузеры, но требует использования драйверов.
*   **Playwright**: Современная альтернатива Selenium, разработанная для скорости и надежности, с поддержкой Chromium, Firefox и WebKit.
*   **Scrapy**: Мощный фреймворк на Python для крупномасштабного веб-скрейпинга, но он не обрабатывает сайты, интенсивно использующие JavaScript, "из коробки".
*   **Web Scraper APIs, Web Unlocker, SERP API**: Готовые API-решения для извлечения структурированных данных из сотен доменов или обхода защит.

---
